{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jihad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jihad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jihad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Optional but helps wordnet work better in some languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChatbotModel – Neural Network for Intent Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a fully connected feedforward neural network designed for classifying user input into chatbot intent categories.  \n",
    "It consists of two hidden layers with ReLU activations and dropout regularization, and an output layer producing logits.\n",
    "\n",
    "- Input: Bag-of-words or vectorized user input\n",
    "- Output: Logits for each intent class\n",
    "- Loss function: CrossEntropyLoss (includes softmax internally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple fully connected feedforward neural network for intent classification.\n",
    "\n",
    "    Architecture:\n",
    "        - Input layer\n",
    "        - Two hidden layers (with ReLU + Dropout)\n",
    "        - Output layer (raw logits for CrossEntropyLoss)\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Number of input features (e.g., vocabulary size)\n",
    "        output_size (int): Number of output classes (e.g., number of intent tags)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "\n",
    "        # First fully connected layer: input_size → 128 hidden units\n",
    "        self.fully_connected_1 = nn.Linear(input_size, 128)\n",
    "        # Second fully connected layer: 128 → 64\n",
    "        self.fully_connected_2 = nn.Linear(128, 64)\n",
    "        # Output layer: 64 → number of intent classes\n",
    "        self.fully_connected_3 = nn.Linear(64, output_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, input_size)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output logits of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        x = self.relu(self.fully_connected_1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.relu(self.fully_connected_2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fully_connected_3(x)  # No softmax here; use CrossEntropyLoss\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChatbotAssistant Class – Handles Intents, Vocabulary & Dataset Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is responsible for:\n",
    "- Loading and parsing the `intents.json` file\n",
    "- Creating a vocabulary from the user patterns\n",
    "- Associating patterns with intent tags\n",
    "- Preparing training data (`X`, `y`) for the model\n",
    "- Optionally handling special function mappings per intent (like launching code)\n",
    "\n",
    "It is the logic brain behind the chatbot's \"understanding\" phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Methods – Tokenization, Lemmatization & Bag-of-Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section includes:\n",
    "- Tokenizing and lemmatizing input patterns\n",
    "- Creating the vocabulary\n",
    "- Building bag-of-words vectors\n",
    "- Parsing the full `intents.json` file to map patterns to tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Preparation – Features and Labels**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method prepares the training dataset (`X`, `y`) by converting tokenized patterns into bag-of-words vectors and mapping each to its corresponding intent index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training, Saving & Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block includes:\n",
    "- `train_model`: trains the neural network on your dataset\n",
    "- `save_model` / `load_model`: persists and reloads the trained model\n",
    "- `process_message`: performs inference and returns a relevant chatbot response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotAssistant:\n",
    "    \"\"\"\n",
    "    A class to load, preprocess, train, and use a chatbot model based on an intents JSON file.\n",
    "\n",
    "    Responsibilities:\n",
    "        - Load and parse intents.\n",
    "        - Tokenize and lemmatize input patterns.\n",
    "        - Build vocabulary and tag list.\n",
    "        - Create training data (X, y).\n",
    "        - Train, save, and load a model.\n",
    "        - Process user input to return a relevant response.\n",
    "\n",
    "    Args:\n",
    "        intents_file (str): Path to the JSON file defining intents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, intents_path):\n",
    "        self.model = None                          # Trained model\n",
    "        self.intents_path = intents_path           # Path to intents JSON\n",
    "\n",
    "        self.documents = []                        # List of (tokens, tag)\n",
    "        self.vocabulary = []                       # Set of unique words\n",
    "        self.intents = []                          # List of intent tags\n",
    "        self.intents_responses = {}                # tag → responses\n",
    "\n",
    "        self.X = None                              # Feature matrix\n",
    "        self.y = None                              # Label vector\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_and_lemmatize(text):\n",
    "        \"\"\"\n",
    "        Tokenizes and lemmatizes a given text input using NLTK.\n",
    "\n",
    "        Args:\n",
    "            text (str): A raw input phrase.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Lowercased, lemmatized word tokens.\n",
    "        \"\"\"\n",
    "        lemmatizer = nltk.WordNetLemmatizer()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return [lemmatizer.lemmatize(word.lower()) for word in tokens]\n",
    "\n",
    "    def bag_of_words(self, tokenized_sentence):\n",
    "        \"\"\"\n",
    "        Converts a tokenized sentence into a binary bag-of-words vector.\n",
    "\n",
    "        Args:\n",
    "            tokenized_sentence (List[str]): Preprocessed tokens.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: A binary list representing presence/absence of vocab words.\n",
    "        \"\"\"\n",
    "        return [1 if word in tokenized_sentence else 0 for word in self.vocabulary]\n",
    "\n",
    "    def parse_intents(self):\n",
    "        \"\"\"\n",
    "        Loads and processes the intents JSON file.\n",
    "        If the same tag appears multiple times, assigns unique names by appending _1, _2, etc.\n",
    "        \"\"\"\n",
    "        lemmatizer = nltk.WordNetLemmatizer()\n",
    "        tag_counts = {}  # To track and rename duplicate tags\n",
    "\n",
    "        if os.path.exists(self.intents_path):\n",
    "            with open(self.intents_path, 'r', encoding='utf-8') as f:\n",
    "                intents_data = json.load(f)\n",
    "\n",
    "            for intent in intents_data['intents']:\n",
    "                base_tag = intent['tag']\n",
    "                # Create unique tag if it’s already been used\n",
    "                if base_tag not in tag_counts:\n",
    "                    tag_counts[base_tag] = 1\n",
    "                    tag = base_tag\n",
    "                else:\n",
    "                    tag_counts[base_tag] += 1\n",
    "                    tag = f\"{base_tag}_{tag_counts[base_tag]}\"\n",
    "\n",
    "                # Store tag and its responses\n",
    "                self.intents.append(tag)\n",
    "                self.intents_responses[tag] = intent['responses']\n",
    "\n",
    "                # Process each pattern into bag-of-words format\n",
    "                for pattern in intent['patterns']:\n",
    "                    pattern_words = self.tokenize_and_lemmatize(pattern)\n",
    "                    self.vocabulary.extend(pattern_words)\n",
    "                    self.documents.append((pattern_words, tag))\n",
    "\n",
    "            self.vocabulary = sorted(set(self.vocabulary))\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Converts all tokenized patterns into numerical BoW features and labels.\n",
    "        \"\"\"\n",
    "        bags = []\n",
    "        indices = []\n",
    "\n",
    "        for document in self.documents:\n",
    "            words = document[0]\n",
    "            bag = self.bag_of_words(words)\n",
    "\n",
    "            intent_index = self.intents.index(document[1])\n",
    "\n",
    "            bags.append(bag)\n",
    "            indices.append(intent_index)\n",
    "\n",
    "        self.X = np.array(bags, dtype=np.uint8)\n",
    "        self.y = np.array(indices)\n",
    "\n",
    "    def train_model(self, batch_size, lr, epochs):\n",
    "        \"\"\"\n",
    "        Trains the chatbot model using a feedforward network.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Training batch size.\n",
    "            lr (float): Learning rate.\n",
    "            epochs (int): Number of training epochs.\n",
    "        \"\"\"\n",
    "        X_tensor = torch.tensor(self.X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(self.y, dtype=torch.long)\n",
    "\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.model = ChatbotModel(self.X.shape[1], len(self.intents)) \n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: Loss: {running_loss / len(loader):.4f}\")\n",
    "\n",
    "    def save_model(self, model_path, dimensions_path):\n",
    "        \"\"\"\n",
    "        Saves the model weights and its architecture details.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): File to save model weights.\n",
    "            dimensions_path (str): File to save input/output sizes.\n",
    "        \"\"\"\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "        with open(dimensions_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'input_size': self.X.shape[1],\n",
    "                'output_size': len(self.intents)\n",
    "            }, f)\n",
    "\n",
    "    def load_model(self, model_path, dimensions_path):\n",
    "        \"\"\"\n",
    "        Loads a trained model and its input/output dimensions.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to model weights.\n",
    "            dimensions_path (str): Path to metadata JSON file.\n",
    "        \"\"\"\n",
    "        with open(dimensions_path, 'r') as f:\n",
    "            dimensions = json.load(f)\n",
    "\n",
    "        self.model = ChatbotModel(dimensions['input_size'], dimensions['output_size'])\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "        self.model.eval()\n",
    "\n",
    "    def process_message(self, input_message):\n",
    "        \"\"\"\n",
    "        Processes a user message:\n",
    "        - Converts to BoW vector\n",
    "        - Predicts intent\n",
    "        - Returns a random response from predicted tag\n",
    "\n",
    "        Args:\n",
    "            input_message (str): User's message.\n",
    "\n",
    "        Returns:\n",
    "            str | None: A chatbot response.\n",
    "        \"\"\"\n",
    "        words = self.tokenize_and_lemmatize(input_message)\n",
    "        bag = self.bag_of_words(words)\n",
    "\n",
    "        bag_tensor = torch.tensor([bag], dtype=torch.float32)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(bag_tensor)\n",
    "\n",
    "        predicted_class_index = torch.argmax(predictions, dim=1).item()\n",
    "        predicted_intent = self.intents[predicted_class_index]\n",
    "\n",
    "        responses = self.intents_responses.get(predicted_intent)\n",
    "        return random.choice(responses) if responses else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the Chatbot – CLI Inference Loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script loads the trained model and lets the user chat with the assistant in the terminal.\n",
    "Use `/quit` to exit the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Chatbot is ready! Type your message below (or type /quit to exit):\n",
      "\n",
      "🤖 Bot: I want to study Mandarin and international relations .\n",
      "🤖 Bot: Thank you . I ’ d better get going . I don ’ t want to be late for lunch . Mom would worry .\n",
      "🤖 Bot: I ’ Ve been to all of them .\n",
      "🤖 Bot: Yes , I do . But my characters are very bad .\n",
      "🤖 Bot: I wish to move up to higher positions with acquisition of more experience in the future .\n",
      "🤖 Bot: You just said your English needs work , yes ?\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #assistant = ChatbotAssistant('dailydialog_intents.json')\n",
    "    #assistant.parse_intents()\n",
    "    #assistant.prepare_data()\n",
    "    #assistant.train_model(batch_size=8, lr=0.001, epochs=100)\n",
    "\n",
    "    #assistant.save_model('chatbot_model.pth', 'dimensions.json')\n",
    "\n",
    "    assistant = ChatbotAssistant('dailydialog_intents.json')\n",
    "    assistant.parse_intents()\n",
    "    assistant.load_model('chatbot_model.pth', 'dimensions.json')\n",
    "\n",
    "    print(\"🤖 Chatbot is ready! Type your message below (or type /quit to exit):\\n\")\n",
    "\n",
    "    # Interactive chat loop\n",
    "    while True:\n",
    "        message = input(\"🗣️ You: \")\n",
    "\n",
    "        if message.strip().lower() == '/quit':\n",
    "            print(\"👋 Chatbot session ended. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        response = assistant.process_message(message)\n",
    "\n",
    "        if response:\n",
    "            print(f\"🤖 Bot: {response}\")\n",
    "        else:\n",
    "            print(\"🤖 Bot: I'm not sure how to respond to that.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
