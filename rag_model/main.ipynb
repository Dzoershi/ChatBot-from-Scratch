{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95dbbaa6",
   "metadata": {},
   "source": [
    "# Building a Semantic Chatbot with SBERT and Together.ai API\n",
    "\n",
    "This notebook implements a hybrid chatbot that combines semantic search using SBERT (Sentence-BERT) and response generation using a large language model (LLM) via the Together.ai API. The system retrieves the most relevant example interactions and uses them as context for producing natural, human-like responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a499c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd307eb7",
   "metadata": {},
   "source": [
    "## Load Together.ai API Key\n",
    "\n",
    "We use the `python-dotenv` package to securely load the API key for Together.ai from a `.env` file.\n",
    "This key is required to authenticate requests to the LLM generation endpoint.\n",
    "\n",
    "If the key is not found, the script raises an error to prevent unauthorized access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16dba8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "\n",
    "if TOGETHER_API_KEY is None:\n",
    "    raise ValueError(\"‚ùå API key not loaded. Check your .env file name or path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cfa154",
   "metadata": {},
   "source": [
    "## SemanticChatbot Class\n",
    "\n",
    "This class defines the main logic of our semantic chatbot system. It combines two powerful components:\n",
    "\n",
    "- **SBERT (Sentence-BERT)** for encoding user queries and pre-defined patterns into dense semantic vectors.\n",
    "- **Together.ai** (via API) for generating natural, context-aware responses based on the most relevant examples.\n",
    "\n",
    "### Main Functionalities:\n",
    "- **Data Loading**: Parses a JSON file containing user patterns and response sets.\n",
    "- **Embedding Generation**: Encodes all patterns into SBERT embeddings and caches them for faster reuse.\n",
    "- **Semantic Retrieval**: Retrieves the top-k semantically similar patterns to the user input.\n",
    "- **Prompt Construction**: Formats the retrieved examples as a few-shot prompt.\n",
    "- **Response Generation**: Sends the prompt to Together.ai's LLM to synthesize a coherent, fluent response.\n",
    "\n",
    "This hybrid approach gives our chatbot the retrieval quality of semantic search with the natural language fluency of large language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07fdbb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticChatbot:\n",
    "    \"\"\"\n",
    "    A semantic chatbot using SBERT for retrieval and Together.ai for generation.\n",
    "\n",
    "    Attributes:\n",
    "        intents_path (str): Path to the JSON file containing intent data.\n",
    "        model_name (str): SentenceTransformer model name for embeddings.\n",
    "        embeddings_path (str): Path to store or load precomputed embeddings.\n",
    "        together_model_name (str): Name of the Together.ai model to use for generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        intents_path,\n",
    "        model_name='all-MiniLM-L6-v2',\n",
    "        embeddings_path=\"embeddings.pt\",\n",
    "        together_model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "    ):\n",
    "        self.intents_path = intents_path\n",
    "        self.embeddings_path = embeddings_path\n",
    "        self.together_model_name = together_model_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "        self.patterns = []\n",
    "        self.responses = []\n",
    "        self.embeddings = None\n",
    "\n",
    "        self._load_data()\n",
    "        self._load_or_build_embeddings()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "        Loads the intent data from the specified JSON file and extracts\n",
    "        the patterns and associated responses.\n",
    "        \"\"\"\n",
    "        with open(self.intents_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.patterns = [item[\"pattern\"] for item in data[\"intents\"]]\n",
    "        self.responses = [item[\"responses\"] for item in data[\"intents\"]]\n",
    "\n",
    "    def _load_or_build_embeddings(self):\n",
    "        \"\"\"\n",
    "        Loads precomputed embeddings from disk or builds them from patterns\n",
    "        using a SentenceTransformer model if not found.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.embeddings_path):\n",
    "            print(f\"Loading embeddings from {self.embeddings_path}...\")\n",
    "            self.embeddings = torch.load(self.embeddings_path)\n",
    "        else:\n",
    "            print(\"Generating embeddings using SBERT...\")\n",
    "            self.embeddings = self.model.encode(self.patterns, convert_to_tensor=True)\n",
    "            torch.save(self.embeddings, self.embeddings_path)\n",
    "            print(f\"Embeddings saved to {self.embeddings_path}\")\n",
    "\n",
    "    def build_prompt(self, user_input, top_k_hits):\n",
    "        \"\"\"\n",
    "        Constructs a prompt to send to the language model using the top-k most\n",
    "        relevant pattern-response examples.\n",
    "\n",
    "        Args:\n",
    "            user_input (str): The message from the user.\n",
    "            top_k_hits (List[Dict]): List of hits from semantic search.\n",
    "\n",
    "        Returns:\n",
    "            str: Prompt formatted for generation.\n",
    "        \"\"\"\n",
    "        examples = \"\"\n",
    "        for i, hit in enumerate(top_k_hits[:3]):\n",
    "            idx = hit[\"corpus_id\"]\n",
    "            question = self.patterns[idx]\n",
    "            answer = random.choice(self.responses[idx])\n",
    "            examples += f\"User: {question}\\nBot: {answer}\\n\\n\"\n",
    "\n",
    "        prompt = (\n",
    "            \"You are a helpful assistant. Below are a few example conversations.\\n\"\n",
    "            \"Use them to respond to the final user input in a concise and natural way.\\n\\n\"\n",
    "            f\"{examples}\"\n",
    "            f\"User: {user_input}\\nBot:\"\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "    def generate_with_together(self, prompt, max_tokens=150):\n",
    "        \"\"\"\n",
    "        Generates a response using Together.ai API given a prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): Prompt text including context and user question.\n",
    "            max_tokens (int): Maximum number of tokens to generate.\n",
    "\n",
    "        Returns:\n",
    "            str: Generated response.\n",
    "        \"\"\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        data = {\n",
    "            \"model\": self.together_model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"stop\": [\"User:\", \"Bot:\"]\n",
    "        }\n",
    "\n",
    "        response = requests.post(\"https://api.together.xyz/v1/completions\", headers=headers, json=data)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(\"Error from Together API:\", response.text)\n",
    "            return \"Sorry, I had a problem generating a response.\"\n",
    "\n",
    "        return response.json()[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    def get_response(self, user_input, top_k=5):\n",
    "        \"\"\"\n",
    "        Generates a semantic chatbot response based on the top-k closest\n",
    "        examples from the training patterns.\n",
    "\n",
    "        Args:\n",
    "            user_input (str): Message from the user.\n",
    "            top_k (int): Number of similar examples to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            str: Response from the chatbot.\n",
    "        \"\"\"\n",
    "        query_embedding = self.model.encode(user_input, convert_to_tensor=True)\n",
    "        hits = util.semantic_search(query_embedding, self.embeddings, top_k=top_k)[0]\n",
    "\n",
    "        if not hits:\n",
    "            return \"I'm not sure how to respond.\"\n",
    "\n",
    "        prompt = self.build_prompt(user_input, hits)\n",
    "        return self.generate_with_together(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5aff6281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading the semantic chatbot using Together.ai...\n",
      "Loading embeddings from embeddings_final.pt...\n",
      "\n",
      "üí¨ Chatbot is ready! Type a message below (or /quit to exit)\n",
      "\n",
      "\n",
      "üì® You said: Hello, how are you ? \n",
      "ü§ñ Bot replied: I'm doing well, thank you. I'm just here to help answer your questions. How can I assist you today?\n",
      "\n",
      "\n",
      "üì® You said: How is the weather today ? \n",
      "ü§ñ Bot replied: It's sunny and a bit windy .\n",
      "\n",
      "\n",
      "üì® You said: \n",
      "ü§ñ Bot replied: Thank you for your understanding .\n",
      "\n",
      "\n",
      "üì® You said: What do you think of the political and economical state of the world right now ? \n",
      "ü§ñ Bot replied: I believe that the current political climate is causing a lot of uncertainty and unpredictability in the global economy . It is very important for world leaders to work together to address these challenges and find solutions .\n",
      "\n",
      "\n",
      "üì® You said: Thank you, that's all for me.\n",
      "ü§ñ Bot replied: Okay, have a great day!\n",
      "\n",
      "üëã Goodbye!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Initialize the semantic chatbot with the dataset and precomputed embeddings\n",
    "    print(\"ü§ñ Loading the semantic chatbot using Together.ai...\")\n",
    "\n",
    "    bot = SemanticChatbot(\n",
    "        intents_path=\"C:/Users/jihad/Desktop/ChatBot-from-Scratch/data_dailydialog/dialogues_final_format.json\",\n",
    "        embeddings_path=\"embeddings_final.pt\"  # This path should match where embeddings are saved\n",
    "    )\n",
    "\n",
    "    print(\"\\nüí¨ Chatbot is ready! Type a message below (or /quit to exit)\\n\")\n",
    "\n",
    "    conversation_log = []\n",
    "\n",
    "    while True:\n",
    "        message = input(\"üó£Ô∏è You: \")\n",
    "        if message.strip().lower() == \"/quit\":\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Generate response\n",
    "        response = bot.get_response(message)\n",
    "\n",
    "        # Save and display the full exchange\n",
    "        conversation_log.append((message, response))\n",
    "        print(f\"\\nüì® You said: {message}\")\n",
    "        print(f\"ü§ñ Bot replied: {response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
